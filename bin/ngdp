#!/usr/bin/env python
import os, sys; sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # noqa isort:skip
import json
import re
import sqlite3
from datetime import datetime
from glob import iglob
from io import BytesIO, StringIO
from itertools import chain
from typing import List, Set
from urllib.parse import urlparse

import click
import toml
from humanize import naturalsize
from tabulate import tabulate
from tqdm import tqdm

from keg import Keg, psv
from keg.archive import ArchiveGroup, ArchiveIndex
from keg.blte import verify_blte_data
from keg.cdn import DEFAULT_CONFIG_PATH, CacheableCDNWrapper
from keg.exceptions import IntegrityVerificationError, NetworkError
from keg.http import StateCache
from keg.utils import partition_hash, verify_data


DEFAULT_REMOTE_PREFIX = "http://us.patch.battle.net:1119/"


def looks_like_md5(s: str) -> bool:
	return bool(re.match(r"[0-9a-f]{32}", s))


def pretty_psv(psvfile, tablefmt="psql"):
	return tabulate([
		[cell.replace(" ", "\n") for cell in row]
		for row in psvfile
	], headers=psvfile.header, tablefmt=tablefmt)


class AmbiguousVersionError(click.ClickException):
	def __init__(self, msg, hints):
		super().__init__(msg)
		self._hints = hints

	def format_message(self):
		hints = "\n    ".join([""] + self._hints)
		return self.message + "\n\nThe candidates are:" + hints


class App:
	def __init__(self, ngdp_dir: str) -> None:
		self.ngdp_path = os.path.abspath(ngdp_dir)
		self.objects_path = os.path.join(self.ngdp_path, "objects")
		self.response_cache_dir = os.path.join(self.ngdp_path, "responses")
		self.db_path = os.path.join(self.ngdp_path, "keg.db")
		self.force_cdn = ""
		self.init_config()

		if os.path.exists(self.ngdp_path):
			self.db = sqlite3.connect(self.db_path)
		else:
			self.db = None

	@property
	def preferred_cdns(self) -> List[str]:
		return self.config["keg"].get("preferred_cdns", [])

	@property
	def remotes(self) -> List[str]:
		return self.config.get("remotes", [])

	@property
	def verify(self) -> bool:
		return self.config["keg"].get("verify-integrity", True)

	@property
	def fetchable_remotes(self) -> List[str]:
		for remote in self.remotes:
			if self.config["remotes"][remote].get("default-fetch"):
				yield remote

	def _download(self, message: str, iterable: Set[str], callback):
		if not iterable:
			print(message, "Up-to-date.")
			return

		bar = tqdm(unit="files", ncols=0, leave=False, total=len(iterable))

		for key in iterable:
			bar.set_description(message + " " + key)
			callback(key, verify=self.verify)
			bar.update()
		bar.close()

		print(message, "Done.")

	def init_config(self):
		self.config_path = os.path.join(self.ngdp_path, "keg.conf")

		if os.path.exists(self.config_path):
			with open(self.config_path, "r") as f:
				self.config = toml.load(f)
			assert "keg" in self.config
			assert self.config["keg"].get("config_version") == 1

			assert "ngdp" in self.config
			assert self.config["ngdp"].get("hash_function") == "md5"
		else:
			self.config = {}

	def init_repo(self):
		if not os.path.exists(self.ngdp_path):
			os.makedirs(self.ngdp_path)
			click.echo(f"Initialized in {self.ngdp_path}")
		else:
			click.echo(f"Reinitialized in {self.ngdp_path}")

		if not os.path.exists(self.config_path):
			self.config["keg"] = {
				"config_version": 1,
				"preferred_cdns": [],
				"default-remote-prefix": DEFAULT_REMOTE_PREFIX,
				"verify-integrity": True,
			}
			self.config["ngdp"] = {
				"hash_function": "md5",
			}
			self.save_config()

		self.db = sqlite3.connect(self.db_path)
		self.db.execute("""
			CREATE TABLE IF NOT EXISTS responses (
				remote text,
				path text,
				timestamp int64,
				digest text
			)
		""")

		self.db.execute("""
			CREATE TABLE IF NOT EXISTS blobs (
				remote text,
				key text,
				row int,
				Region text,
				InstallBlobMD5 text,
				GameBlobMD5 text
			)
		""")

		self.db.execute("""
			CREATE TABLE IF NOT EXISTS cdns (
				remote text,
				key text,
				row int,
				Name text,
				Path text,
				Hosts text,
				Servers text,
				ConfigPath text
			)
		""")

		for table_name in ("versions", "bgdl"):
			self.db.execute("""
				CREATE TABLE IF NOT EXISTS "%s" (
					remote text,
					key text,
					row int,
					BuildConfig text,
					BuildID text,
					CDNConfig text,
					KeyRing text,
					ProductConfig text,
					Region text,
					VersionsName text
				)
			""" % (table_name))

	def save_config(self):
		with open(self.config_path, "w") as f:
			toml.dump(self.config, f)

	def _choose_cdn(self, cdns):
		if self.force_cdn:
			url = urlparse(self.force_cdn)
			if not url.scheme or not url.netloc or not url.path:
				raise click.ClickException(f"Invalid CDN url: {self.force_cdn}")

			return CacheableCDNWrapper(
				base_dir=self.objects_path,
				server=f"{url.scheme}://{url.netloc}",
				path=url.path,
				config_path=DEFAULT_CONFIG_PATH,
			)

		if not cdns:
			raise click.ClickException("No CDNs available. Use --cdn to specify one.")

		available_cdns = ", ".join(cdn.name for cdn in cdns)
		cdns_lookup = {cdn.name.lower(): cdn for cdn in cdns}

		for preferred_cdn in self.preferred_cdns:
			cdn_name = preferred_cdn.lower()
			if cdn_name in cdns_lookup:
				cdn = cdns_lookup[cdn_name]
				break
		else:
			cdn = cdns[0]

		tqdm.write(f"Using {cdn.name} (available: {available_cdns})")

		assert cdn.all_servers

		return CacheableCDNWrapper(
			base_dir=self.objects_path,
			server=cdn.all_servers[0],
			path=cdn.path,
			config_path=cdn.config_path,
		)

	def _clean_remote(self, remote: str) -> str:
		if "://" not in remote:
			return self.config["keg"].get(
				"default-remote-prefix", DEFAULT_REMOTE_PREFIX
			) + remote
		return remote

	def find_version(self, version: str, remote: str) -> str:
		cursor = self.db.cursor()
		cursor.execute("""
			SELECT distinct(BuildConfig)
			FROM versions
			WHERE
				REMOTE = ? AND
				VersionsName = ? OR BuildID = ?
			GROUP BY BuildConfig
		""", (remote, version, version))
		results = cursor.fetchall()
		if len(results) == 1:
			return results[0][0]
		else:
			raise AmbiguousVersionError(
				f"Version {repr(version)} is ambiguous",
				sorted(set(chain(*results)))
			)

	def get_remote(self, remote: str) -> Keg:
		return Keg(remote, cache_dir=self.response_cache_dir, cache_db=self.db)

	def fetch(self, remote: str, metadata_only: bool=False):
		remote = self._clean_remote(remote)
		click.echo(f"Fetching {remote}")

		keg = self.get_remote(remote)

		# keg fetch
		# 1. get the version
		# 2. get the cdns
		# 3. pick a cdn..? / keep the other ones as fallback
		# 4. pull the build config
		# 5. pull the cdn config
		# 6. pull the patch config
		# # 7. pull the product config (?)
		# 8. pull encoding file (which we got from build_config), if not resolved yet
		# 9. download all the archive indices
		# 10. do stuff for patch files
		# Finally... resolve all dangling data references

		bar = tqdm(leave=False, total=4, bar_format="{desc}", postfix="")

		# Look up all available CDNs and choose one to use
		bar.set_description_str("Receiving CDN list")
		try:
			cdns = keg.get_cdns()
		except NetworkError:
			raise click.ClickException(f"NGDP repository {remote} not found")

		cdn = self._choose_cdn(cdns)

		# Find all available versions
		bar.set_description_str("Receiving version list")
		versions = keg.get_versions()

		bar.set_description_str("Receiving background download metadata")
		try:
			keg.get_bgdl()
		except NetworkError:
			# bgdl is optional
			pass

		# Update blobs

		bar.set_description_str("Receiving blobs")
		try:
			keg.get_blobs()
		except NetworkError:
			# Optional
			pass
		for blob_name in ("game", "install"):
			bar.set_description_str(f"Receiving {blob_name} blob")
			try:
				keg.get_blob(blob_name)
			except NetworkError:
				# Blobs are optional
				pass

		# Look at all product configs, find encrypted ones
		decryption_keys = {}
		for version in versions:
			product_key = version.product_config
			if not product_key or product_key in decryption_keys:
				pass

			if not cdn.local_cdn.has_config_item(product_key):
				bar.set_description_str(f"Receiving product config: {product_key}")
				cdn.fetch_config_data(product_key, verify=self.verify)

			product_config = cdn.get_product_config(product_key, verify=self.verify)

			# Check if the version is encrypted.
			# Best guess: decryption key name is in the product config.
			decryption_key_name = product_config.get("all", {}).get("config", {}).get(
				"decryption_key_name", ""
			)
			if decryption_key_name and product_key not in decryption_keys:
				tqdm.write(f"Build {version.build_config} ({version.versions_name}) is encrypted")
				tqdm.write(f"Decryption key {repr(decryption_key_name)} required")
				decryption_keys[product_key] = decryption_key_name

		bar.close()

		if decryption_keys:
			click.echo("This repository is encrypted. Key not available. Aborting fetch.")
			return

		config_to_fetch = set()
		for version in versions:
			if not cdn.local_cdn.has_config(version.build_config):
				config_to_fetch.add(version.build_config)
			if not cdn.local_cdn.has_config(version.cdn_config):
				config_to_fetch.add(version.cdn_config)

		self._download("Fetching config...", config_to_fetch, cdn.fetch_config)

		indices_to_fetch = set()
		for version in versions:
			cdn_config = cdn.get_cdn_config(version.cdn_config, verify=self.verify)
			for archive_key in cdn_config.archives:
				if not cdn.local_cdn.has_index(archive_key):
					indices_to_fetch.add(archive_key)

		self._download("Fetching indices...", indices_to_fetch, cdn.fetch_index)

		if metadata_only:
			return

		archives_to_fetch = set()
		loose_files_to_fetch = set()
		patch_files_to_fetch = set()
		for version in versions:
			cdn_config = cdn.get_cdn_config(
				version.cdn_config, verify=self.verify
			)
			# get the archive list

			for archive in cdn_config.archives:
				if not cdn.local_cdn.has_data(archive):
					archives_to_fetch.add(archive)

			# Create the merged archive group
			archive_group = ArchiveGroup(
				cdn_config.archives,
				cdn_config.archive_group,
				cdn,
				verify=self.verify
			)

			build_config = cdn.get_build_config(
				version.build_config, verify=self.verify
			)

			encoding_file = build_config.get_encoding_file(cdn, verify=self.verify)
			if encoding_file:
				# Download loose files
				for key in encoding_file.encoding_keys:
					if not archive_group.has_file(key) and not cdn.local_cdn.has_data(key):
						loose_files_to_fetch.add(key)

			# Download patch files
			if build_config.patch_config:
				patch_config = cdn.get_patch_config(
					build_config.patch_config, verify=self.verify
				)
				for patch_entry in patch_config.patch_entries:
					for old_key, old_size, patch_key, patch_size in patch_entry.pairs:
						if not cdn.local_cdn.has_patch(patch_key):
							patch_files_to_fetch.add(patch_key)

		self._download(
			"Fetching archives...", archives_to_fetch, cdn.download_data
		)
		self._download(
			"Fetching loose files...", loose_files_to_fetch, cdn.download_data
		)
		self._download(
			"Fetching patch files...", patch_files_to_fetch, cdn.fetch_patch
		)

		# whats left?
		# metadata:
		# - root
		# - install
		# - patch archive indices
		# - patch files
		#   - patch manifest
		#   - files referenced by patch


@click.group()
@click.option("--ngdp-dir", default=".ngdp")
@click.option("--cdn")
@click.pass_context
def main(ctx, ngdp_dir, cdn):
	ctx.obj = App(ngdp_dir)
	ctx.obj.force_cdn = cdn


@main.command()
@click.pass_context
def init(ctx):
	ctx.obj.init_repo()


@main.command()
@click.argument("remote")
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch(ctx, remote, metadata_only=True):
	try:
		ctx.obj.fetch(remote, metadata_only)
	except IntegrityVerificationError as e:
		raise click.ClickException(str(e))


@main.command("fetch-all")
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch_all(ctx, metadata_only=True):
	for remote in ctx.obj.fetchable_remotes:
		ctx.invoke(fetch, remote=remote, metadata_only=metadata_only)


@main.command("fetch-object")
@click.argument("keys", nargs=-1, required=True)
@click.option("--type", default="data", type=click.Choice(["data", "config"]))
@click.pass_context
def fetch_object(ctx, keys, type):
	cdn = ctx.obj._choose_cdn([])
	bar = tqdm(unit="file", ncols=0, leave=False, total=len(keys))

	if type == "data":
		for key in keys:
			msg = f"Fetching {key}..."
			bar.set_description(f"Fetching {key}...")
			if cdn.local_cdn.has_data(key):
				bar.write(f"{msg} Up-to-date.")
			else:
				try:
					cdn.download_data(key, verify=ctx.obj.verify)
					bar.write(f"{msg} OK")
				except NetworkError:
					bar.write(f"{msg} Not found.")

			bar.update()

		bar.close()

	elif type == "config":
		raise NotImplementedError()


@main.group()
@click.pass_context
def remote(ctx):
	if "remotes" not in ctx.obj.config:
		ctx.obj.config["remotes"] = {}


@remote.command("add")
@click.argument("remotes", nargs=-1, required=True)
@click.option("--writeable", is_flag=True)
@click.option("--default-fetch/--no-default-fetch", default=True)
@click.pass_context
def add_remote(ctx, remotes, default_fetch, writeable):
	for remote in remotes:
		remote = ctx.obj._clean_remote(remote)

		if remote in ctx.obj.config["remotes"]:
			raise click.ClickException(f"Remote {remote} already exists")

		ctx.obj.config["remotes"][remote] = {
			"default-fetch": default_fetch,
			"writeable": writeable,
		}
		ctx.obj.save_config()


@remote.command("rm")
@click.argument("remotes", nargs=-1, required=True)
@click.pass_context
def remove_remote(ctx, remotes):
	for remote in remotes:
		remote = ctx.obj._clean_remote(remote)

		try:
			del ctx.obj.config["remotes"][remote]
		except KeyError:
			raise click.ClickException(f"No such remote: {remote}")

		ctx.obj.save_config()


@remote.command("list")
@click.pass_context
def list_remotes(ctx):
	for remote in ctx.obj.remotes:
		print(remote)


@main.command("inspect")
@click.argument("remote")
@click.pass_context
def inspect(ctx, remote):
	remote = ctx.obj._clean_remote(remote)

	cursor = ctx.obj.db.cursor()
	cursor.execute("""
		SELECT
			distinct(BuildConfig), BuildID, VersionsName
		FROM versions
		WHERE
			remote = ?
	""", (remote,))
	results = cursor.fetchall()

	if not results:
		click.echo(f"No known data for {remote}")
		return

	click.echo(f"Remote: {remote}\n")
	click.echo(tabulate(
		results,
		headers=("Build Config", "Build ID", "Version")
	))


@main.command()
@click.argument("remote")
@click.argument("version")
@click.option("--show-tags", is_flag=True)
@click.option("--tags", multiple=True)
@click.pass_context
def install(ctx, remote, version, tags, show_tags):
	remote = ctx.obj._clean_remote(remote)

	if not looks_like_md5(version.lower()):
		key = ctx.obj.find_version(version, remote)
		if not key:
			raise click.ClickException(f"Version not found: {version}")
	else:
		key = version.lower()

	click.echo(f"Checking out {key}...")

	keg = ctx.obj.get_remote(remote)
	cdn = ctx.obj._choose_cdn(keg.get_cached_cdns(remote, ctx.obj.response_cache_dir))

	build_config = cdn.get_build_config(key, verify=ctx.obj.verify)

	install_file = build_config.get_install_file(cdn)
	if not install_file:
		raise click.ClickException("Install file not found")

	if show_tags:
		click.echo("Valid tags:\n")
		table = tabulate(
			[(k, v[0]) for k, v in install_file.tags.items()],
			tablefmt="psql",
			headers=["Tag", "Type"]
		)
		click.secho(table, fg="black", bold=True)
		return

	entries = list(install_file.filter_entries(tags))

	table = tabulate(
		sorted(entries), tablefmt="psql", headers=["Filename", "Digest", "Size"]
	)
	click.secho(table, fg="black", bold=True)

	total_size = sum(k[2] for k in entries)
	click.echo(f"Would use {naturalsize(total_size, binary=True)}")

	num_conflicts = len(entries) - len(set(k[0] for k in entries))
	click.echo(f"{num_conflicts} conflicting files")


@main.command("log")
@click.argument("remote")
@click.option("--type", default="versions", type=click.Choice(["versions", "cdns"]))
@click.pass_context
def show_log(ctx, remote, type):
	remote = ctx.obj._clean_remote(remote)
	column = f"/{type}"

	cursor = ctx.obj.db.cursor()
	cursor.execute("""
		SELECT digest, timestamp
		FROM "responses"
		WHERE
			remote = ? AND
			path = ?
		ORDER BY timestamp
	""", (remote, column))
	results = cursor.fetchall()

	state_cache = StateCache(ctx.obj.response_cache_dir)

	last = ""
	for digest, timestamp in results:
		if digest == last:
			# Skip contiguous digests (always only show oldest)
			continue
		last = digest
		click.secho(f"{type} {digest}", fg="yellow")
		click.echo("Date: " + datetime.fromtimestamp(timestamp).isoformat() + "Z")
		click.echo(f"URL: {remote + column}\n")
		if state_cache.exists(type, digest):
			contents = state_cache.read(type, digest)
			table = pretty_psv(psv.load(StringIO(contents)))
			click.secho(table, fg="black", bold=True)
		else:
			click.secho("(not available)", fg="red")
		click.echo()


@main.command("show")
@click.argument("remote")
@click.argument("object")
@click.pass_context
def show_object(ctx, remote, object):
	remote = ctx.obj._clean_remote(remote)
	if object.lower() == "buildconfig":
		column = "BuildConfig"
	elif object.lower() == "cdnconfig":
		column = "CDNConfig"
	elif object.lower() == "productconfig":
		column = "ProductConfig"
	else:
		raise click.ClickException(f"Unknown object type: {object}")

	cursor = ctx.obj.db.cursor()

	cursor.execute(f"""
		SELECT distinct("{column}")
		FROM versions
		WHERE
			remote = ? AND
			"{column}" != ''
	""", (remote, ))

	results = cursor.fetchall()
	if not results:
		raise click.ClickException(f"No known {column} for {remote}.")

	for res, in results:
		click.echo(f"{column}: {res}")


@main.command("fsck")
@click.option("--delete", is_flag=True)
@click.pass_context
def fsck(ctx, delete):
	def _get_objects(patterns):
		return sorted(
			f for pattern in patterns
			for f in iglob(os.path.join(ctx.obj.ngdp_path, pattern))
		)

	objects = _get_objects((
		"responses/bgdl/*/*/*",
		"responses/blob/*/*/*/*",
		"responses/blobs/*/*/*",
		"responses/cdns/*/*/*",
		"responses/versions/*/*/*",
		"objects/config/*/*/*",
		"objects/configs/data/*/*/*",
		"objects/data/*/*/*",
		"objects/patch/*/*/*",
	))

	fail = 0
	bar = tqdm(unit="object", ncols=0, leave=True, total=len(objects))
	deleted_str = " (deleted)" if delete else ""
	for path in objects:
		bar.update()
		base_path = path[len(ctx.obj.ngdp_path) + 1:]  # Strip the full path to .ngdp
		key = os.path.basename(path)
		bar.set_description(f"Checking objects: {key} ({base_path})")
		if key.endswith(".keg_temp"):
			tqdm.write(f"Dangling file: {path}{deleted_str}", sys.stderr)
			if delete:
				os.remove(path)
			continue

		is_data = base_path.startswith("objects/data/")
		if len(key) != 32 and not (is_data and key.endswith(".index")):
			if delete:
				os.remove(path)
			tqdm.write(f"Unknown file: {path}{deleted_str}", sys.stderr)
			continue

		with open(path, "rb") as f:
			try:
				if is_data:
					if path.endswith(".index"):
						key = key[:-len(".index")]
						f.seek(-28, os.SEEK_END)
						verify_data("archive index", f.read(), key, verify=True)
					elif os.path.exists(path + ".index"):
						with open(path + ".index", "rb") as index_file:
							index = ArchiveIndex(index_file.read(), key, verify=False)
							for item_key, item_size, item_offset in index.items:
								f.seek(item_offset)
								verify_blte_data(BytesIO(f.read(item_size)), item_key)

					else:
						verify_blte_data(f, key)
				else:
					verify_data("object on disk", f.read(), key, verify=True)
			except IntegrityVerificationError as e:
				if delete:
					os.remove(path)
				tqdm.write(f"Integrity error: {path}{deleted_str}", sys.stderr)
				tqdm.write(str(e), sys.stderr)
				fail += 1
			except Exception as e:
				import traceback
				tqdm.write(f"Error while verifying {path}", sys.stderr)
				tqdm.write(traceback.format_exc(), sys.stderr)

	bar.set_description("Checking responses: Done.")
	bar.close()

	if fail:
		tqdm.write(f"{fail} bad objects!")
		exit(1)


@main.command("parse-index")
@click.argument("key_or_path")
@click.option("--format", default="table", type=click.Choice(["table", "json"]))
@click.pass_context
def parse_index(ctx, key_or_path, format):
	def get_actual_path(path):
		if os.path.exists(path):
			return path

		if looks_like_md5(key_or_path):
			real_path = os.path.join(
				ctx.obj.objects_path,
				"data",
				partition_hash(key_or_path) + ".index"
			)
			if os.path.exists(real_path):
				return real_path

	real_path = get_actual_path(key_or_path)
	if not real_path:
		raise click.ClickException(f"No such file or object: {key_or_path}")

	key = os.path.basename(os.path.splitext(real_path)[0])
	if not looks_like_md5(key):
		click.echo(f"WARNING: Invalid key name {repr(key)}", file=sys.stderr)

	with open(real_path, "rb") as f:
		data = f.read()

	archive_index = ArchiveIndex(data, key, verify=ctx.obj.verify)

	items = list(archive_index.items)

	if format == "table":
		output = tabulate(items, tablefmt="psql", headers=("Key", "Size", "Offset"))
	elif format == "json":
		output = json.dumps(items, indent="\t")

	click.secho(output, fg="black", bold=True)


if __name__ == "__main__":
	main()
