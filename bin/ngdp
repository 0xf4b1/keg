#!/usr/bin/env python
import os, sys; sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # noqa isort:skip
import json
import re
import sqlite3
from datetime import datetime
from glob import iglob
from io import BytesIO
from itertools import chain
from typing import List, Set, Tuple
from urllib.parse import urlparse

import click
import toml
from humanize import naturalsize
from tabulate import tabulate
from tqdm import tqdm

from keg import Keg, blte, psv
from keg.archive import ArchiveGroup, ArchiveIndex
from keg.cdn import DEFAULT_CONFIG_PATH, CacheableCDNWrapper
from keg.exceptions import IntegrityVerificationError, NetworkError
from keg.http import StateCache, Versions
from keg.utils import partition_hash, verify_data


DEFAULT_REMOTE_PREFIX = "http://us.patch.battle.net:1119/"


def looks_like_md5(s: str) -> bool:
	return bool(re.match(r"[0-9a-f]{32}", s))


class AmbiguousVersionError(click.ClickException):
	def __init__(self, msg, hints):
		super().__init__(msg)
		self._hints = hints

	def format_message(self):
		hints = "\n    ".join([""] + self._hints)
		return self.message + "\n\nThe candidates are:" + hints


class App:
	def __init__(self, ngdp_dir: str) -> None:
		self.ngdp_path = os.path.abspath(ngdp_dir)
		self.objects_path = os.path.join(self.ngdp_path, "objects")
		self.response_cache_dir = os.path.join(self.ngdp_path, "responses")
		self.db_path = os.path.join(self.ngdp_path, "keg.db")
		self.force_cdn = ""
		self.init_config()

		if os.path.exists(self.ngdp_path):
			self.db = sqlite3.connect(self.db_path)
		else:
			self.db = None

	@property
	def preferred_cdns(self) -> List[str]:
		return self.config["keg"].get("preferred_cdns", [])

	@property
	def remotes(self) -> List[str]:
		return self.config.get("remotes", [])

	@property
	def verify(self) -> bool:
		return self.config["keg"].get("verify-integrity", True)

	@property
	def fetchable_remotes(self) -> List[str]:
		for remote in self.remotes:
			if self.config["remotes"][remote].get("default-fetch"):
				yield remote

	def tabulate(self, table, headers=None) -> str:
		if isinstance(table, psv.PSVFile):
			headers = table.header
			table = [
				[cell.replace(" ", "\n") for cell in row] for row in table
			]

		if self.table_format == "json":
			if not isinstance(table, list):
				table = list(table)
			return json.dumps(table, indent="\t")

		return tabulate(table, headers=headers, tablefmt=self.table_format)

	def _download(self, message: str, iterable: Set[str], callback):
		if not iterable:
			print(message, "Up-to-date.")
			return

		bar = tqdm(unit="files", ncols=0, leave=False, total=len(iterable))

		for key in iterable:
			bar.set_description(message + " " + key)
			try:
				callback(key, verify=self.verify)
			except NetworkError as e:
				tqdm.write(f"WARNING: {str(e)}", sys.stderr)
			bar.update()
		bar.close()

		print(message, "Done.")

	def init_config(self):
		self.config_path = os.path.join(self.ngdp_path, "keg.conf")

		if os.path.exists(self.config_path):
			with open(self.config_path, "r") as f:
				self.config = toml.load(f)
			assert "keg" in self.config
			assert self.config["keg"].get("config_version") == 1

			assert "ngdp" in self.config
			assert self.config["ngdp"].get("hash_function") == "md5"
		else:
			self.config = {}

	def init_repo(self):
		if not os.path.exists(self.ngdp_path):
			os.makedirs(self.ngdp_path)
			click.echo(f"Initialized in {self.ngdp_path}")
		else:
			click.echo(f"Reinitialized in {self.ngdp_path}")

		if not os.path.exists(self.config_path):
			self.config["keg"] = {
				"config_version": 1,
				"preferred_cdns": [],
				"default-remote-prefix": DEFAULT_REMOTE_PREFIX,
				"verify-integrity": True,
			}
			self.config["ngdp"] = {
				"hash_function": "md5",
			}
			self.save_config()

		self.db = sqlite3.connect(self.db_path)
		self.db.execute("""
			CREATE TABLE IF NOT EXISTS responses (
				remote text,
				path text,
				timestamp int64,
				digest text,
				source int
			)
		""")

		self.db.execute("""
			CREATE TABLE IF NOT EXISTS blobs (
				remote text,
				key text,
				row int,
				Region text,
				InstallBlobMD5 text,
				GameBlobMD5 text
			)
		""")

		self.db.execute("""
			CREATE TABLE IF NOT EXISTS cdns (
				remote text,
				key text,
				row int,
				Name text,
				Path text,
				Hosts text,
				Servers text,
				ConfigPath text
			)
		""")

		for table_name in ("versions", "bgdl"):
			self.db.execute("""
				CREATE TABLE IF NOT EXISTS "%s" (
					remote text,
					key text,
					row int,
					BuildConfig text,
					BuildID int,
					CDNConfig text,
					KeyRing text,
					ProductConfig text,
					Region text,
					VersionsName text
				)
			""" % (table_name))

	def save_config(self):
		with open(self.config_path, "w") as f:
			toml.dump(self.config, f)

	def _choose_cdn(self, cdns):
		if self.force_cdn:
			url = urlparse(self.force_cdn)
			if not url.scheme or not url.netloc or not url.path:
				raise click.ClickException(f"Invalid CDN url: {self.force_cdn}")

			return CacheableCDNWrapper(
				base_dir=self.objects_path,
				server=f"{url.scheme}://{url.netloc}",
				path=url.path,
				config_path=DEFAULT_CONFIG_PATH,
			)

		if not cdns:
			raise click.ClickException("No CDNs available. Use --cdn to specify one.")

		available_cdns = ", ".join(cdn.name for cdn in cdns)
		cdns_lookup = {cdn.name.lower(): cdn for cdn in cdns}

		for preferred_cdn in self.preferred_cdns:
			cdn_name = preferred_cdn.lower()
			if cdn_name in cdns_lookup:
				cdn = cdns_lookup[cdn_name]
				break
		else:
			cdn = cdns[0]

		tqdm.write(f"Using {cdn.name} (available: {available_cdns})")

		assert cdn.all_servers

		return CacheableCDNWrapper(
			base_dir=self.objects_path,
			server=cdn.all_servers[0],
			path=cdn.path,
			config_path=cdn.config_path,
		)

	def _clean_remote(self, remote: str) -> str:
		if "://" not in remote:
			return self.config["keg"].get(
				"default-remote-prefix", DEFAULT_REMOTE_PREFIX
			) + remote
		return remote

	def find_version(self, version: str, remote: str) -> Tuple[str, str]:
		cursor = self.db.cursor()
		cursor.execute("""
			SELECT distinct(BuildConfig), CDNConfig
			FROM versions
			WHERE
				REMOTE = ? AND
				VersionsName = ? OR BuildID = ? OR BuildConfig = ?
			GROUP BY BuildConfig
		""", (remote, version, version, version))
		results = cursor.fetchall()
		if not results:
			raise click.ClickException(f"Version not found: {version}")
		elif len(results) == 1:
			return results[0]
		else:
			raise AmbiguousVersionError(
				f"Version {repr(version)} is ambiguous",
				sorted(set(chain(*results)))
			)

	def get_remote(self, remote: str) -> Keg:
		return Keg(remote, cache_dir=self.response_cache_dir, cache_db=self.db)

	def fetch_stateful_data(self, keg: Keg):
		bar = tqdm(leave=False, total=4, bar_format="{desc}", postfix="")

		# Look up all available CDNs and choose one to use
		bar.set_description_str("Receiving CDN list")
		try:
			cdns = keg.get_cdns()
		except NetworkError:
			raise click.ClickException(f"NGDP repository {remote} not found")

		cdn = self._choose_cdn(cdns)

		# Find all available versions
		bar.set_description_str("Receiving version list")
		versions = keg.get_versions()

		bar.set_description_str("Receiving background download metadata")
		try:
			keg.get_bgdl()
		except NetworkError:
			# bgdl is optional
			pass

		# Update blobs

		bar.set_description_str("Receiving blobs")
		try:
			keg.get_blobs()
		except NetworkError:
			# Optional
			pass
		for blob_name in ("game", "install"):
			bar.set_description_str(f"Receiving {blob_name} blob")
			try:
				keg.get_blob(blob_name)
			except NetworkError:
				# Blobs are optional
				pass

		# Look at all product configs, find encrypted ones
		decryption_keys = {}
		for version in versions:
			product_key = version.product_config
			if not product_key or product_key in decryption_keys:
				continue

			if not cdn.local_cdn.has_config_item(product_key):
				bar.set_description_str(f"Receiving product config: {product_key}")
				cdn.fetch_config_data(product_key, verify=self.verify)

			product_config = cdn.get_product_config(product_key, verify=self.verify)

			# Check if the version is encrypted.
			# Best guess: decryption key name is in the product config.
			decryption_key_name = product_config.get("all", {}).get("config", {}).get(
				"decryption_key_name", ""
			)
			if decryption_key_name and product_key not in decryption_keys:
				tqdm.write(f"Build {version.build_config} ({version.versions_name}) is encrypted")
				tqdm.write(f"Decryption key {repr(decryption_key_name)} required")
				decryption_keys[product_key] = decryption_key_name

		bar.close()

		return cdn, versions, decryption_keys

	def fetch(self, cdn, versions, metadata_only: bool=False):
		# Regions are often duplicated per region.
		# In order not to do all the work several times, we dedupe first.
		deduped_versions = {
			f"{v.build_config}:{v.cdn_config}:{v.product_config}": v for v in versions
		}
		versions = list(deduped_versions.values())

		config_to_fetch = set()
		for version in versions:
			if not cdn.local_cdn.has_config(version.build_config):
				config_to_fetch.add(version.build_config)
			if not cdn.local_cdn.has_config(version.cdn_config):
				config_to_fetch.add(version.cdn_config)

		self._download("Fetching config...", config_to_fetch, cdn.fetch_config)

		indices_to_fetch = set()
		patch_indices_to_fetch = set()
		for version in versions:
			try:
				cdn_config = cdn.get_cdn_config(version.cdn_config, verify=self.verify)
			except NetworkError:
				tqdm.write(f"WARNING: Cannot get CDN config {version.cdn_config}. Skipping.")
			else:
				for archive_key in cdn_config.archives:
					if not cdn.local_cdn.has_index(archive_key):
						indices_to_fetch.add(archive_key)

			try:
				build_config = cdn.get_build_config(version.build_config, verify=self.verify)
			except NetworkError as e:
				tqdm.write(f"WARNING: Cannot get build config {version.build_config}.")
				continue

			# Download patch files
			if build_config.patch_config:
				key = build_config.patch_config
				try:
					patch_config = cdn.get_patch_config(key, verify=self.verify)
				except NetworkError as e:
					tqdm.write(f"WARNING: Cannot get patch config {key}. Skipping.", sys.stderr)
				else:
					for patch_entry in patch_config.patch_entries:
						for old_key, old_size, patch_key, patch_size in patch_entry.pairs:
							if not cdn.local_cdn.has_patch_index(patch_key):
								patch_indices_to_fetch.add(patch_key)

		self._download("Fetching indices...", indices_to_fetch, cdn.fetch_index)
		self._download("Fetching patch indices...", indices_to_fetch, cdn.fetch_patch_index)

		if metadata_only:
			return

		archives_to_fetch = set()
		loose_files_to_fetch = set()
		patch_files_to_fetch = set()
		bar = tqdm(leave=False, total=4, bar_format="{desc}", postfix="")
		for version in versions:
			msg = f"Preparing version {version.build_config}"
			bar.set_description_str(msg)
			try:
				cdn_config = cdn.get_cdn_config(version.cdn_config, verify=self.verify)
			except NetworkError:
				tqdm.write(f"WARNING: Cannot get CDN config {version.cdn_config}. Skipping.")
				continue

			# get the archive list
			for archive in cdn_config.archives:
				if not cdn.local_cdn.has_data(archive):
					archives_to_fetch.add(archive)

			# Create the merged archive group
			bar.set_description_str(f"{msg}: Creating merged archive")
			archive_group = ArchiveGroup(
				cdn_config.archives,
				cdn_config.archive_group,
				cdn,
				verify=self.verify
			)

			bar.set_description_str(f"{msg}: Retrieving build configuration")
			try:
				build_config = cdn.get_build_config(
					version.build_config, verify=self.verify
				)
			except NetworkError as e:
				tqdm.write(f"WARNING: Cannot get build config {version.build_config}.")
				continue

			bar.set_description_str(f"{msg}: Retrieving encoding file")
			try:
				encoding_file = build_config.get_encoding_file(cdn, verify=self.verify)
			except NetworkError:
				tqdm.write(f"WARNING: Cannot get encoding file {build_config.encodings}.")
				encoding_file = None

			if encoding_file:
				# Download loose files
				for key in encoding_file.encoding_keys:
					if not archive_group.has_file(key) and not cdn.local_cdn.has_data(key):
						loose_files_to_fetch.add(key)

			# Download patch files
			if build_config.patch_config:
				bar.set_description_str(f"{msg}: Retrieving patch configuration")
				try:
					patch_config = cdn.get_patch_config(
						build_config.patch_config, verify=self.verify
					)
				except NetworkError:
					tqdm.write(f"WARNING: Cannot get patch config {key}. Skipping.", sys.stderr)
				else:
					for patch_entry in patch_config.patch_entries:
						for old_key, old_size, patch_key, patch_size in patch_entry.pairs:
							if not cdn.local_cdn.has_patch(patch_key):
								patch_files_to_fetch.add(patch_key)

		bar.close()

		self._download(
			"Fetching archives...", archives_to_fetch, cdn.download_data
		)
		self._download(
			"Fetching loose files...", loose_files_to_fetch, cdn.download_data
		)
		self._download(
			"Fetching patch files...", patch_files_to_fetch, cdn.fetch_patch
		)

		# whats left?
		# metadata:
		# - root
		# - install
		# - patch files
		#   - patch manifest
		#   - files referenced by patch


@click.group()
@click.option("--ngdp-dir", default=".ngdp")
@click.option("--cdn")
@click.option("--table-format", default="psql")
@click.pass_context
def main(ctx, ngdp_dir, cdn, table_format):
	ctx.obj = App(ngdp_dir)
	ctx.obj.force_cdn = cdn
	ctx.obj.table_format = table_format


@main.command()
@click.pass_context
def init(ctx):
	ctx.obj.init_repo()


@main.command()
@click.argument("remote")
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch(ctx, remote, metadata_only=True):
	remote = ctx.obj._clean_remote(remote)
	click.echo(f"Fetching {remote}")
	keg = ctx.obj.get_remote(remote)
	cdn, versions, decryption_keys = ctx.obj.fetch_stateful_data(keg)
	if decryption_keys:
		click.echo("This repository is encrypted. Key not available. Aborting fetch.")
		return

	try:
		ctx.obj.fetch(cdn, versions, metadata_only)
	except IntegrityVerificationError as e:
		raise click.ClickException(str(e))


@main.command("fetch-all")
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def fetch_all(ctx, metadata_only=True):
	for remote in ctx.obj.fetchable_remotes:
		ctx.invoke(fetch, remote=remote, metadata_only=metadata_only)


@main.command("force-fetch")
@click.argument("remote")
@click.argument("version-keys", nargs=-1, required=True)
@click.option("--metadata-only", is_flag=True)
@click.pass_context
def force_fetch(ctx, remote, version_keys, metadata_only):
	remote = ctx.obj._clean_remote(remote)
	keg: Keg = ctx.obj.get_remote(remote)
	cdn = ctx.obj._choose_cdn(keg.get_cached_cdns(remote, ctx.obj.response_cache_dir))

	for version_key in version_keys:
		psvfile = StateCache(ctx.obj.response_cache_dir).read_psv("/versions", version_key)
		versions = [Versions(row) for row in psvfile]

		try:
			ctx.obj.fetch(cdn, versions, metadata_only)
		except IntegrityVerificationError as e:
			raise click.ClickException(str(e))


@main.command("fetch-object")
@click.argument("keys", nargs=-1, required=True)
@click.option("--type", default="data", type=click.Choice(["data", "config"]))
@click.pass_context
def fetch_object(ctx, keys, type):
	cdn = ctx.obj._choose_cdn([])
	bar = tqdm(unit="file", ncols=0, leave=False, total=len(keys))

	if type == "data":
		for key in keys:
			msg = f"Fetching {key}..."
			bar.set_description(f"Fetching {key}...")
			if cdn.local_cdn.has_data(key):
				bar.write(f"{msg} Up-to-date.")
			else:
				try:
					cdn.download_data(key, verify=ctx.obj.verify)
					bar.write(f"{msg} OK")
				except NetworkError:
					bar.write(f"{msg} Not found.")

			bar.update()

		bar.close()

	elif type == "config":
		raise NotImplementedError()


@main.command("add")
@click.argument("paths", nargs=-1, required=True)
@click.option("--type", required=True, type=click.Choice([
	"cdns",
	"config",
	"data-index",
	"patch",
	"patch-index",
	"versions",
]))
@click.option("--remote")
@click.pass_context
def add_object(ctx, paths, type, remote):
	def _ingest_move(path: str, ngdp_path: str) -> None:
		click.echo(f"{path} => {ngdp_path}")
		dirname = os.path.dirname(ngdp_path)
		if not os.path.exists(dirname):
			os.makedirs(dirname)
		os.rename(path, ngdp_path)

	for path in paths:
		key = os.path.basename(path)
		with open(path, "rb") as f:
			data = f.read()

		if type == "config":
			verify_data("config file", data, key, verify=ctx.obj.verify)
			# Sanity check
			assert data.startswith(b"# ")

			ngdp_path = os.path.join(ctx.obj.objects_path, "config", partition_hash(key))
			_ingest_move(path, ngdp_path)

		elif type in ("cdns", "versions"):
			verify_data(f"{type} response", data, key, verify=ctx.obj.verify)
			if not remote:
				raise click.BadParameter(remote, param_hint="--remote")

			# Sanity check
			if type == "cdns":
				assert data.startswith(b"Name!STRING:0|")
			elif type == "versions":
				assert data.startswith(b"Region!STRING:0|")

			remote = ctx.obj._clean_remote(remote)
			keg = ctx.obj.get_remote(remote)

			cursor = ctx.obj.db.cursor()
			psvfile = psv.loads(data.decode())
			keg.cache_psv(psvfile, key, f"/{type}", cursor)
			ctx.obj.db.commit()

			ngdp_path = os.path.join(
				ctx.obj.response_cache_dir, type, partition_hash(key)
			)
			_ingest_move(path, ngdp_path)

		elif type in ("data-index", "patch-index"):
			key = os.path.splitext(key)[0]
			verify_data(type, data[-28:], key, verify=ctx.obj.verify)
			obj_dir = "data" if type == "data-index" else "patch"
			ngdp_path = os.path.join(
				ctx.obj.objects_path, obj_dir, partition_hash(key) + ".index"
			)
			_ingest_move(path, ngdp_path)

		elif type == "patch":
			verify_data("patch file", data, key, verify=ctx.obj.verify)
			assert data.startswith(b"ZBSDIFF1")

			ngdp_path = os.path.join(
				ctx.obj.objects_path, "patch", partition_hash(key)
			)
			_ingest_move(path, ngdp_path)


@main.group()
@click.pass_context
def remote(ctx):
	if "remotes" not in ctx.obj.config:
		ctx.obj.config["remotes"] = {}


@remote.command("add")
@click.argument("remotes", nargs=-1, required=True)
@click.option("--writeable", is_flag=True)
@click.option("--default-fetch/--no-default-fetch", default=True)
@click.pass_context
def add_remote(ctx, remotes, default_fetch, writeable):
	for remote in remotes:
		remote = ctx.obj._clean_remote(remote)

		if remote in ctx.obj.config["remotes"]:
			raise click.ClickException(f"Remote {remote} already exists")

		ctx.obj.config["remotes"][remote] = {
			"default-fetch": default_fetch,
			"writeable": writeable,
		}
		ctx.obj.save_config()


@remote.command("rm")
@click.argument("remotes", nargs=-1, required=True)
@click.pass_context
def remove_remote(ctx, remotes):
	for remote in remotes:
		remote = ctx.obj._clean_remote(remote)

		try:
			del ctx.obj.config["remotes"][remote]
		except KeyError:
			raise click.ClickException(f"No such remote: {remote}")

		ctx.obj.save_config()


@remote.command("list")
@click.pass_context
def list_remotes(ctx):
	for remote in ctx.obj.remotes:
		print(remote)


@main.command("inspect")
@click.argument("remote")
@click.pass_context
def inspect(ctx, remote):
	remote = ctx.obj._clean_remote(remote)

	cursor = ctx.obj.db.cursor()
	cursor.execute("""
		SELECT
			distinct(BuildConfig), BuildID, VersionsName
		FROM versions
		WHERE
			remote = ?
		ORDER BY BuildID ASC
	""", (remote,))
	results = cursor.fetchall()

	if not results:
		click.echo(f"No known data for {remote}")
		return

	click.echo(f"Remote: {remote}\n")
	click.echo(ctx.obj.tabulate(
		results, headers=("Build Config", "Build ID", "Version"),
	))


@main.command()
@click.argument("remote")
@click.argument("version")
@click.argument("outdir", default=".")
@click.option("--show-tags", is_flag=True)
@click.option("--tags", multiple=True)
@click.option("--dry-run", "--dryrun", is_flag=True)
@click.pass_context
def install(ctx, remote, version, outdir, tags, show_tags, dryrun):
	remote = ctx.obj._clean_remote(remote)

	# version can be a BuildName, BuildID or BuildConfig
	build_config_key, cdn_config_key = ctx.obj.find_version(version, remote)

	click.echo(f"Checking out {build_config_key}...")

	keg = ctx.obj.get_remote(remote)
	cdn = ctx.obj._choose_cdn(keg.get_cached_cdns(remote, ctx.obj.response_cache_dir))

	build_config = cdn.get_build_config(build_config_key, verify=ctx.obj.verify)

	install_file = build_config.get_install_file(cdn)
	if not install_file:
		raise click.ClickException("Install file not found")

	if show_tags:
		click.echo("Valid tags:\n")
		table = ctx.obj.tabulate(
			[(k, v[0]) for k, v in install_file.tags.items()],
			headers=("Tag", "Type")
		)
		click.secho(table, fg="black", bold=True)
		return

	entries = sorted(install_file.filter_entries(tags))

	if dryrun:
		table = ctx.obj.tabulate(
			entries, headers=["Filename", "Digest", "Size"]
		)
		click.secho(table, fg="black", bold=True)

	total_size = sum(k[2] for k in entries)
	click.echo(f"Total size: {naturalsize(total_size, binary=True)}")

	num_conflicts = len(entries) - len(set(k[0] for k in entries))
	click.echo(f"{num_conflicts} conflicting files")

	install_dir = os.path.abspath(outdir)
	click.echo(f"Installation directory: {install_dir}")

	encoding_file = build_config.get_encoding_file(cdn)
	if not encoding_file:
		raise click.ClickException("No encoding file found. Cannot proceed.")

	cdn_config = cdn.get_cdn_config(cdn_config_key, verify=ctx.obj.verify)
	archive_group = ArchiveGroup(
		cdn_config.archives,
		cdn_config.archive_group,
		cdn,
		verify=ctx.obj.verify
	)

	if dryrun:
		return

	prev_filename, prev_key = "", ""
	bar = tqdm(unit="file", ncols=0, leave=False, total=len(entries))
	for filename, key, size in entries:
		bar.update()
		if filename == prev_filename:
			# Skips over potential conflicts
			if key != prev_key:
				tqdm.write(f"WARNING: Unresolved conflict for {filename}", sys.stderr)
			continue

		prev_filename, prev_key = filename, key
		file_path = os.path.join(install_dir, filename)

		if os.path.exists(file_path):
			raise click.ClickException(f"{file_path} already exists. Not overwriting.")

		bar.set_description(f"Installing {filename} ({naturalsize(size)})")

		try:
			encoding_key = encoding_file.find_by_content_key(key)
		except KeyError:
			tqdm.write(f"WARNING: Cannot find {key} ({filename}). Skipping.", sys.stderr)
			continue

		tqdm.write(file_path)

		dirname = os.path.dirname(file_path)
		if not os.path.exists(dirname):
			os.makedirs(dirname)

		if cdn.local_cdn.has_data(encoding_key):
			with cdn.download_data(encoding_key, verify=ctx.obj.verify) as encoded_file:
				decoder = blte.BLTEDecoder(encoded_file, encoding_key, verify=ctx.obj.verify)
				with open(file_path, "wb") as f:
					decoder.decode_and_write(f)
		elif archive_group.has_file(encoding_key):
			data = archive_group.get_file_by_key(encoding_key)
			with open(file_path, "wb") as f:
				f.write(data)
		else:
			raise click.ClickException(
				f"Cannot install {file_path}: Missing data for {encoding_key}. "
				"Try running `ngdp fetch {remote}`."
			)

	bar.close()


@main.command("log")
@click.argument("remote")
@click.option("--type", default="versions", type=click.Choice(["versions", "cdns"]))
@click.pass_context
def show_log(ctx, remote, type):
	remote = ctx.obj._clean_remote(remote)
	column = f"/{type}"

	cursor = ctx.obj.db.cursor()
	cursor.execute("""
		SELECT digest, timestamp
		FROM "responses"
		WHERE
			remote = ? AND
			path = ?
		ORDER BY timestamp
	""", (remote, column))
	results = cursor.fetchall()

	state_cache = StateCache(ctx.obj.response_cache_dir)

	last = ""
	for digest, timestamp in results:
		if digest == last:
			# Skip contiguous digests (always only show oldest)
			continue
		last = digest
		click.secho(f"{type} {digest}", fg="yellow")
		click.echo("Date: " + datetime.fromtimestamp(timestamp).isoformat() + "Z")
		click.echo(f"URL: {remote + column}\n")
		if state_cache.exists(type, digest):
			contents = state_cache.read(type, digest)
			table = ctx.obj.tabulate(psv.loads(contents))
			click.secho(table, fg="black", bold=True)
		else:
			click.secho("(not available)", fg="red")
		click.echo()


@main.command("show")
@click.argument("remote")
@click.argument("object")
@click.pass_context
def show_object(ctx, remote, object):
	remote = ctx.obj._clean_remote(remote)
	if object.lower() == "buildconfig":
		column = "BuildConfig"
	elif object.lower() == "cdnconfig":
		column = "CDNConfig"
	elif object.lower() == "productconfig":
		column = "ProductConfig"
	else:
		raise click.ClickException(f"Unknown object type: {object}")

	cursor = ctx.obj.db.cursor()

	cursor.execute(f"""
		SELECT distinct("{column}")
		FROM versions
		WHERE
			remote = ? AND
			"{column}" != ''
	""", (remote, ))

	results = cursor.fetchall()
	if not results:
		raise click.ClickException(f"No known {column} for {remote}.")

	for res, in results:
		click.echo(f"{column}: {res}")


@main.command("fsck")
@click.option("--delete", is_flag=True)
@click.pass_context
def fsck(ctx, delete):
	def _get_objects(patterns):
		return sorted(
			f for pattern in patterns
			for f in iglob(os.path.join(ctx.obj.ngdp_path, pattern))
		)

	objects = _get_objects((
		"responses/bgdl/*/*/*",
		"responses/blob/*/*/*/*",
		"responses/blobs/*/*/*",
		"responses/cdns/*/*/*",
		"responses/versions/*/*/*",
		"objects/config/*/*/*",
		"objects/configs/data/*/*/*",
		"objects/data/*/*/*",
		"objects/patch/*/*/*",
	))

	fail = 0
	bar = tqdm(unit="object", ncols=0, leave=True, total=len(objects))
	deleted_str = " (deleted)" if delete else ""
	for path in objects:
		bar.update()
		base_path = path[len(ctx.obj.ngdp_path) + 1:]  # Strip the full path to .ngdp
		key = os.path.basename(path)
		bar.set_description(f"Checking objects: {key} ({base_path})")
		if key.endswith(".keg_temp"):
			tqdm.write(f"Dangling file: {path}{deleted_str}", sys.stderr)
			if delete:
				os.remove(path)
			continue

		is_data = base_path.startswith("objects/data/")
		if len(key) != 32 and not (is_data and key.endswith(".index")):
			if delete:
				os.remove(path)
			tqdm.write(f"Unknown file: {path}{deleted_str}", sys.stderr)
			continue

		with open(path, "rb") as f:
			try:
				if is_data:
					if path.endswith(".index"):
						key = key[:-len(".index")]
						f.seek(-28, os.SEEK_END)
						verify_data("archive index", f.read(), key, verify=True)
					elif os.path.exists(path + ".index"):
						with open(path + ".index", "rb") as index_file:
							index = ArchiveIndex(index_file.read(), key, verify=False)
							for item_key, item_size, item_offset in index.items:
								f.seek(item_offset)
								blte.verify_blte_data(BytesIO(f.read(item_size)), item_key)

					else:
						blte.verify_blte_data(f, key)
				else:
					verify_data("object on disk", f.read(), key, verify=True)
			except IntegrityVerificationError as e:
				if delete:
					os.remove(path)
				tqdm.write(f"Integrity error: {path}{deleted_str}", sys.stderr)
				tqdm.write(str(e), sys.stderr)
				fail += 1
			except Exception as e:
				import traceback
				tqdm.write(f"Error while verifying {path}", sys.stderr)
				tqdm.write(traceback.format_exc(), sys.stderr)

	bar.set_description("Checking responses: Done.")
	bar.close()

	if fail:
		tqdm.write(f"{fail} bad objects!")
		exit(1)


@main.command("parse-index")
@click.argument("key_or_path")
@click.pass_context
def parse_index(ctx, key_or_path):
	def get_actual_path(path):
		if os.path.exists(path):
			return path

		if looks_like_md5(key_or_path):
			real_path = os.path.join(
				ctx.obj.objects_path,
				"data",
				partition_hash(key_or_path) + ".index"
			)
			if os.path.exists(real_path):
				return real_path

	real_path = get_actual_path(key_or_path)
	if not real_path:
		raise click.ClickException(f"No such file or object: {key_or_path}")

	key = os.path.basename(os.path.splitext(real_path)[0])
	if not looks_like_md5(key):
		click.echo(f"WARNING: Invalid key name {repr(key)}", file=sys.stderr)

	with open(real_path, "rb") as f:
		data = f.read()

	archive_index = ArchiveIndex(data, key, verify=ctx.obj.verify)

	click.secho(
		ctx.obj.tabulate(archive_index.items, headers=("Key", "Size", "Offset")),
		fg="black", bold=True
	)


if __name__ == "__main__":
	main()
